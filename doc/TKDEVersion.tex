%% bare_jrnl_compsoc.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% Computer Society journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/


\documentclass[10pt,journal,compsoc]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[10pt,journal,compsoc]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.


% *** CITATION PACKAGES ***
%
\ifCLASSOPTIONcompsoc
  %% IEEE Computer Society needs nocompress option
  %% requires cite.sty v4.0 or later (November 2003)
  \usepackage[nocompress]{cite}
\else
  % normal IEEE
  \usepackage{cite}
\fi
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.
%
% Note that some packages require special options to format as the Computer
% Society requires. In particular, Computer Society  papers do not use
% compressed citation ranges as is done in typical IEEE papers
% (e.g., [1]-[4]). Instead, they list every citation separately in order
% (e.g., [1], [2], [3], [4]). To get the latter we need to load the cite
% package with the nocompress option which is supported by cite.sty v4.0
% and later. Note also the use of a CLASSOPTION conditional provided by
%% IEEEtran.cls V1.7 and later.





% *** GRAPHICS RELATED PACKAGES ***
%
\usepackage[pdftex]{graphicx}
% declare the path(s) where your graphic files are
\graphicspath{{./img/}}
% and their extensions so you won't have to specify these with
% every instance of \includegraphics
\DeclareGraphicsExtensions{.pdf,.jpeg,.png}
%\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{./img/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
%\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
%\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex






% *** MATH PACKAGES ***
%
\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath
\usepackage{amsthm,amssymb}
\usepackage{stmaryrd}




% TOD: Use algorithmic to rewrite all algorithms
% *** SPECIALIZED LIST PACKAGES ***
%
% \usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx
%\usepackage{algorithmic}
\usepackage{algorithmicx}
\usepackage{algorithm}
\usepackage{algpseudocode}



% *** ALIGNMENT PACKAGES ***
%
\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


%% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
\ifCLASSOPTIONcompsoc
  \usepackage[caption=false,font=footnotesize,labelfont=sf,textfont=sf]{subfig}
\else
  \usepackage[caption=false,font=footnotesize]{subfig}
\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a sans serif font rather
% than the serif font used in traditional IEEE formatting and thus the need
% to invoke different subfig.sty package options depending on whether
% compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
%% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
%% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.





% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

% user commands
%% -------
%% Scalar
%% -------
\newcommand{\Sca}[3]{{#1}^{(#2)}_{i_#2#3}}%Scalar with superscript and subscript
% entries in factor matrices
\newcommand{\anr}[2]{\Sca{a}{#1}{#2}}
% entries in extension factor matrices
\newcommand{\enr}[2]{\Sca{e}{#1}{\V{#2}}}
% score for {#1}-turn
\newcommand{\score}[1]{\xi_{\V{i},#1}}
%% -------
%% Tensor
%% -------
\newcommand{\T}[1]{\mathcal{#1}}
\newcommand{\KT}[1]{\llbracket #1 \rrbracket}
%% -------
%% Vector
%% -------
\newcommand{\V}[1]{{\boldsymbol{{\MakeLowercase{#1}}}}}
% Vector with superscript and subscript
\newcommand{\ColVec}[3]{\V{#1}^{(#2)}_{#3}}
\newcommand{\NormColA}[2]{\norm{\ColVec{a}{#1}{*#2}}{1}}
\newcommand{\NormColE}[2]{\norm{\ColVec{e}{#1}{*\V{#2}}}{1}}
\newcommand{\RowVecA}[1]{\V{a}^{(#1)}_{i_#1*}}
\newcommand{\ColVecA}[1]{\V{a}^{(#1)}_{*r}}
% others
\newcommand{\coord}{(i_1,i_2,\ldots,i_N)}
\newcommand{\WeightR}{\NormColA{1}{r}\ldots\NormColA{N}{r}}
\newcommand{\predx}{\hat{x}_{\V{i}}}
\newcommand{\predxn}{\hat{x}_{\V{i},n}}

%% -------
%% Matrix
%% -------
\newcommand{\M}[1]{{\textbf{{\MakeUppercase{#1}}}}}
\newcommand{\FacMat}[2]{\M{#1}^{(#2)}}
% p-norm
\newcommand{\norm}[2]{\|#1\|_{#2}}
%% ------------
%% reference
%% ------------
% reference:definition
\newcommand{\Def}[1]{Definition~\ref{def:#1}}
% reference:equation
\newcommand{\Eqn}[1]{Eq.(\ref{eq:#1})}
% reference:figure
\newcommand{\Fig}[1]{Fig.~\ref{fig:#1}}
\newcommand{\Figs}[2]{Fig.~\ref{fig:#1}$\sim$\ref{fig:#2}}
% reference:table
\newcommand{\Table}[1]{Table~\ref{table:#1}}
% reference:lemma
\newcommand{\Lem}[1]{Lemma~\ref{lem:#1}}
% reference:theorem
\newcommand{\Theo}[1]{Theorem~\ref{theo:#1}}
% reference:property
\newcommand{\Prop}[1]{Property~\ref{prop:#1}}
% reference:algorithm
\newcommand{\Alg}[1]{Algorithm~\ref{alg:#1}}
\newcommand{\AlgLine}[2]{line~\ref{line:#2} of Algorithm~\ref{alg:#1}}
\newcommand{\AlgLines}[3]{lines~\ref{line:#2}--\ref{line:#3} of Algorithm~\ref{alg:#1}}

\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}

\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{A Sampling Technique for Approximate Maximum Search in Tensor}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%
%
%\IEEEcompsocitemizethanks is a special \thanks that produces the bulleted
% lists the Computer Society journals use for "first footnote" author
% affiliations. Use \IEEEcompsocthanksitem which works much like \item
% for each affiliation group. When not in compsoc mode,
% \IEEEcompsocitemizethanks becomes like \thanks and
% \IEEEcompsocthanksitem becomes a line break with idention. This
% facilitates dual compilation, although admittedly the differences in the
% desired content of \author between the different types of papers makes a
% one-size-fits-all approach a daunting prospect. For instance, compsoc 
% journal papers have the author affiliations above the "Manuscript
% received ..."  text while in non-compsoc journals this is reversed. Sigh.

\author{ ~Zhi~Lu,%~\IEEEmembership{Member,~IEEE,}
        ~Yang~Hu,%~\IEEEmembership{Member,~IEEE,}
        and~Bing~Zeng,~\IEEEmembership{~Fellow,~IEEE}% <-this % stops a space
\IEEEcompsocitemizethanks{
	\IEEEcompsocthanksitem Zhi Lu, Yang Hu and Bing Zeng are with School of Electronic Engineering, University of Electronic Science and Technology of China.\protect\\
	% note need leading \protect in front of \\ to get a newline within \thanks as
	% \\ is fragile and will error, could use \hfil\break instead.
	E-mail: zhilu@std.uestc.edu.cn, \{yanghu,eezeng\}@uestc.edu.cn
	%\IEEEcompsocthanksitem J. Doe and J. Doe are with Anonymous University.
}% <-this % stops an unwanted space
\thanks{Manuscript received April 19, 2005; revised August 26, 2015.}}

% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
\markboth{IEEE Transactions on Knowledge and Data Engineering,~Vol.~14, No.~8, August~2015}%
{Zhi \MakeLowercase{\textit{et al.}}: A Sampling Technique for Approximate Maximum Search in Tensor}
%\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2015}%
%{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for Computer Society Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.



% The publisher's ID mark at the bottom of the page is less important with
% Computer Society journal papers as those publications place the marks
% outside of the main text columns and, therefore, unlike regular IEEE
% journals, the available text space is not reduced by their presence.
% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% or like this to get the Computer Society new two part style.
%\IEEEpubid{\makebox[\columnwidth]{\hfill 0000--0000/00/\$00.00~\copyright~2015 IEEE}%
%\hspace{\columnsep}\makebox[\columnwidth]{Published by the IEEE Computer Society\hfill}}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark (Computer Society jorunal
% papers don't need this extra clearance.)



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}


% for Computer Society papers, we must declare the abstract and index terms
%% PRIOR to the title within the \IEEEtitleabstractindextext IEEEtran
% command as these need to go into the title area created by \maketitle.
% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\IEEEtitleabstractindextext{
\begin{abstract}
Factorization models have been extensively used for
recovering the missing entries of a matrix or tensor.
However, direct computing all entries
using the learned factorization models is prohibitive
when the size of matrix/tensor is large.
On the other hand, in many applications,
such as collaborative filtering,
we are only interested in a few entries that are the largest among all.
In this work, we propose a sampling-based approach for finding the top entries of a tensor
which is decomposed by the CANDECOMP/PARAFAC model.
We develop an algorithm to sample the entries with probabilities proportional to their values.
We further extend it to make the sampling proportional to the $k$-th power of the values,
amplifying the focus on the top ones.
We provide theoretical analysis of the sampling algorithm and evaluate its performance on several real-world data sets.
Experimental results indicate that the proposed approach is orders of magnitude faster than exhaustive computing.
When applied to the special case of searching in a matrix,
it also requires fewer samples than the state-of-the-art method.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Maximum Search, Tensor Decomposition, Information Retrial
\end{IEEEkeywords}}


% make the title area
\maketitle


% To allow for easy dual compilation without having to reenter the
% abstract/keywords data, the \IEEEtitleabstractindextext text will
% not be used in maketitle, but will appear (i.e., to be "transported")
% here as \IEEEdisplaynontitleabstractindextext when the compsoc 
% or transmag modes are not selected <OR> if conference mode is selected 
% - because all conference papers position the abstract like regular
% papers do.
\IEEEdisplaynontitleabstractindextext
% \IEEEdisplaynontitleabstractindextext has no effect when using
% compsoc or transmag under a non-conference mode.



% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle


\IEEEraisesectionheading{\section{Introduction}\label{sec:introduction}}

% Computer Society journal (but not conference!) papers do something unusual
% with the very first section heading (almost always called "Introduction").
% They place it ABOVE the main text! IEEEtran.cls does not automatically do
% this for you, but you can achieve this effect with the provided
% \IEEEraisesectionheading{} command. Note the need to keep any \label that
% is to refer to the section immediately after \section in the above as
% \IEEEraisesectionheading puts \section within a raised box.

% The very first letter is a 2 line initial drop letter followed
% by the rest of the first word in caps (small caps for compsoc).
% 
% form to use if the first word consists of a single letter:
% \IEEEPARstart{A}{demo} file is ....
% 
% form to use if you need the single drop letter followed by
% normal text (unknown if ever used by the IEEE):
% \IEEEPARstart{A}{}demo file is ....
% 
% Some journals put the first two words in caps:
% \IEEEPARstart{T}{his demo} file is ....
% 
% Here we have the typical use of a "T" for an initial drop letter
% and "HIS" in caps to complete the first word.
%\IEEEPARstart{T}{his} demo file is intended to serve as a ``starter file''
%for IEEE Computer Society journal papers produced under \LaTeX\ using
%% IEEEtran.cls version 1.8b and later.
%% You must have at least 2 lines in the paragraph with the drop letter
%% (should never be an issue)
%I wish you the best of success.
%
%\hfill mds
% 
%\hfill August 26, 2015

% needed in second column of first page if using \IEEEpubid
%\IEEEpubidadjcol



% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure}

% Note that the IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.
% However, the Computer Society has been known to put floats at the bottom.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a 
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.
% Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
% labels, the optional argument to \subfloat must be present. If a
% subcaption is not desired, just leave its contents blank,
% e.g., \subfloat[].


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table and, given that table
% captions serve much like titles, are usually capitalized except for words
% such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to
% and up, which are usually not capitalized unless they are the first or
% last word of the caption. Table text will default to \footnotesize as
% the IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrL_aheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively. 
% Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.

\IEEEPARstart{M}{atrix} and tensor completion has received considerable attention in recent years.
Many problems in application can be formulated as recovering the missing entries of a matrix or tensor. In some scenarios, such as image and video in-painting~\cite{Ankita14},
all lost entries are needed to be filled in. In some others, however, it is difficult and also unnecessary to recover them all. For example, in recommender systems, the size of prediction matrix or tensor, which is determined by the numbers of users and items, is usually rather large and needs to be reconstructed from latent representation of users and items. It is computational expensive to compute all of the unknown values, even given one particular user. On the other hand, for recommendation purpose, we are only interested in a few entries that are the largest within a sub-array of the matrix or tensor.
%Moreover, a good recommendation system should avoid overspecialization[need a cite] and one simply way to increase the serendipity is to "inject a note of randomness"[need a cite].
The largest entries of a matrix/tensor are not only the central concerns for personalized recommendation, but also meaningful in many other cases. In a similarity matrix, the top entries correspond to pairs of items that are most similar, which are of interest for applications like link prediction in graph~\cite{LibenNowell07,DunlavyKolda11}, duplicate detection~\cite{Ke2010}, information retrieval~\cite{Salton03IR} and the electroencephalography analysis\cite{Estienne2001Multi}. 

For those mentioned fields in literature, where the model of tensor decomposition is employed, the final tensor is always not apparent directly but the decomposition forms are and reconstruction is followed among those decomposed matrices. Besides the actual value in final tensor is not critical and the minority often owns relative high values. So in this work, we study the problem of efficiently identifying the top entries within a tensor without exhaustive computing. 

The tensor, as a multi-way generalization of the matrix, has been exploited more and more recently. Take the recommender systems for example. While traditional focus is on the user-item matrix/tensor which is required for data representation in many emerging settings such as context-aware recommendation~\cite{BPTF,Adomavicius2011}, where contextual information like time and location is considered, and set-based collaborative filtering recommender\cite{HuYiLa15,Rendle_PITF,KoYe09} where the object to be recommended is a set of items or suits that interact with each other. Following the most common paradigm, we assume that the tensor can be decomposed into some factors, which can be estimated from the observed entries by some learning algorithms.

Specifically, we focus on the CANDECOMP/PARAFAC decomposition model~\cite{KoBa09,AcarYener09}. CP decomposition is one kind of tensor decomposition technique that widely used for exploring and extracting the underlying structure of multi-way data including a interpretable factorization for time dimension. Given a $N$-order tensor $\T{X}\in\mathbb{R}^{L_1\times \cdots\times L_N}$, CP decomposition approximates it by $N$ factor matrices $\FacMat{A}{1},\FacMat{A}{2},\ldots,\FacMat{A}{N}$, such that:
\begin{align}
\label{eq:CPDecomposition}
\T{X}&\approx\KT{\FacMat{A}{1},\FacMat{A}{2},\cdots,\FacMat{A}{N}} \\ \notag
&=\sum_{r=1}^R\ColVecA{1}\circ\ColVecA{2}\circ\cdots\circ\ColVecA{N}
\end{align}
where each factor matrix $\FacMat{A}{n}=[\ColVec{a}{n}{*1}\ColVec{a}{n}{*2}\cdots\ColVec{a}{n}{*R}], n=1,\ldots,N$ is of size $L_n\times R$ with $\ColVec{a}{n}{*r}\in\mathbb{R}^{L_n}$ being the $r$-th column. The symbol ``$\circ$'' represents the vector outer product. $R$ is the tensor rank, indicating the number of latent factors. Element-wise,~\Eqn{CPDecomposition} is written as:
\begin{align}
\label{eq:CPValue}
x_\V{i} \approx \sum_{r=1}^{R}\anr{1}{r}\anr{2}{r}\cdots\anr{N}{r}
\end{align}
where $\V{i}$ is short for the index tuple $(i_1,i_2,\ldots,i_N)$.


\subsection{Problem Reformation and Related Works}
The tensor models relationships between each dimensions and are trained into factor matrices with small $R$ among many interesting fields. And the retrial stage calls for the reconstruction of whole tensor. So given $N$ factor matrices $\textbf{A}^{(1)},\ldots,\textbf{A}^{(N)}$, the explicit CP decomposition of a prediction tensor $\T{X}$, and a parameter $t$, we would like to find $t$ index tuples $\{\V{i}_1,\ldots,\V{i}_t\}$ which correspond to the top-$t$ largest values $x_{\V{i}}$. This problem subsumes many existing problems in the literature.
When $N=2$, it is exactly the MAD (Maximum All-pairs Dot-product Search) problem~\cite{BaPiKoSe15}
that finds the largest entries in the product of two matrices.
And MAD contains the MIPS (Maximum Inner Product Search)~\cite{Cohen97} problem
as the special case with one matrix being a single column. The most obvious approach is to compute the entries exhaustively via the reconstruction of whole tensor.
However, this becomes prohibitive as the sizes of the factor matrices grows.

There is some literature in approximate matrix multiplication.
However these methods are not suited even for MAD, since only a few entries among the millions are of interest. The more efficient solution in  matrix multiplication is to directly search the top ones. This has been extensively studied for the MIPS problem. Popular approaches include LSH (Locality Sensing Hashing)~\cite{Andoni08,ALSH14}, space partition techniques like k-d tree and some sampling-based approaches~\cite{Drineas2006,John15} are also proposed. Recently, Ballard et al. proposed a randomized approach called diamond sampling~\cite{BaPiKoSe15} for the MAD problem. They selected diamonds, i.e. four-cycles, from a weighted tripartite representation of the two factor matrices, with the probability of selecting a diamond corresponding to index pair $(i_1,i_2)$ being proportional to $(\RowVecA{1}\cdot\RowVecA{2})^2$, where $\RowVecA{n}$ is used to represent the $i_n$ row of the factor matrix $\FacMat{A}{1},\FacMat{A}{2}$. For tensor, there hasn't been any study conducted yet.

Inspired by the work of diamond sampling~\cite{BaPiKoSe15}, we apply index sampling methods to the case of tensor, whose entries are computed by the CP decomposition model. We design a strategy to sample the index tuple $\V{i}$ proportional to the magnitude of the corresponding entries. We further extend the basic approach to strengthen the sampling proportional to the $k$-th power of the entries, amplifying the focus on largest ones. For the application of recommender systems, an algorithm that reuse the samples for different users is presented. We also provide theoretical analysis for our sampling algorithms, and derive concentration bounds on the behavior. We evaluate the sampling methods on several real-world data sets. The results show that they are orders of magnitude faster than exact computing. When compared to previous approach for matrix sampling, our methods require much fewer samples. 

\section{The Sampling Method}
In this section, we propose the basic sampling approach for finding the top-$N$ entries in tensor, after which, an advanced one will be introduced.


\subsection{Basic Sampling Method}

\begin{figure*}
	\centering
	\subfloat[]{
		\includegraphics[width=0.3\linewidth]{GraphFactorMatrices_a}
		\label{fig:GraphMatrices:a}
	}\hfil
	\subfloat[]{
		\includegraphics[width=0.3\linewidth]{GraphFactorMatrices_b}
		\label{fig:GraphMatrices:b}
	}\hfil
	\subfloat[]{
		\includegraphics[width=0.3\linewidth]{GraphFactorMatrices_c}
		\label{fig:GraphMatrices:c}
	}
	\caption{Graph representation of factor matrices and sampling mechanism. (a) The graph representation of matrices. (b) Once a node in sampled from core partition, each peripheral partition will has a probability distribution conditioned on that node. (c) A sampled index tuple.}
	\label{fig:GraphMatrices}
\end{figure*}

The idea of randomized approach is to sample the entries in a tensor according to some probability distribution so that the larger one entry is the more probable it is picked. To explain this mechanism, we use a (N+1)-partite graph to represent the factor matrices $\FacMat{A}{1},\ldots,\FacMat{A}{N}$ in \Eqn{CPDecomposition}. 

Consider one of the matrix $\FacMat{A}{n}\in \mathbb{R}^{L_n \times R}$, it is represented by a bipartite graph shown within the dash-dot box in ~\Fig{GraphMatrices:a}. We first use $R$ nodes indexed by $r\in[R]$ to denote the $R$ columns of it, where $[R]$ denotes $\{1,\ldots,R\}$. They constitute the core partition and are also shared by other factor matrices. We then use $L_n$ nodes, indexed by $i_n\in [L_n]$, to represent the rows of $\FacMat{A}{n}$ which constitute a peripheral partition. Edge $(i_n,r)$ exists if the value $\anr{n}{r}$ is nonzero. Beside, each factor matrix is represented by a individual peripheral partition as depicted in~\Fig{GraphMatrices:a}. To improve legibility, only one factor matrix is plotted in detail.

To sample an index tuple $\V{i}$, we sequentially pick each of its indices $i_n$ but starting from the core partition. We first assign appropriate weights to the nodes in core partition. For $r\in[R]$, let
\begin{equation}
w_r = \WeightR
\end{equation}
where $\|\cdot\|$ denotes 1-norm of the vector. We sample the nodes in core partition with probability $w_r/\norm{\V{W}}{1}$. As shown in ~\Fig{GraphMatrices:b}, after one core node indexed by $r$ is chosen, $N$ peripheral nodes, one from each peripheral partition, to be picked conditioned on $r$. And the node in $n$-th partition indexed by $i_n$ is drawn with probability $|\anr{n}{r}|/\norm{\ColVecA{n}}{1}$. After an index tuple $\V{i}=\coord$, as depicted shematically in ~\Fig{GraphMatrices:c}, is obtained, a score is computed as following:
\begin{equation}
\score{\ell}  = sgn(\anr{1}{r}\cdot\anr{2}{r}\cdots\anr{N}{r})
\end{equation}
where $\ell$ denotes the $\ell$-th sample. If the index tuple $\V{i}$ has not been sampled before, a container is created with $\predx = \score{\ell}$. Otherwise, we increase $\predx$ by $\score{\ell}$. The procedure is shown in \Alg{CoreSampling}.

\begin{algorithm}
	\caption{The basic sampling method}
	\label{alg:CoreSampling}
	Given factor matrix $\FacMat{A}{n}\in \mathbb{R}^{L_n\times R}, n = 1,2,\ldots,N$.\\
	Let $s$ be the number of samples.
	\begin{algorithmic}[1]
		\For{$r\in{1,2,\ldots,R}$}
		\State $w_r \leftarrow \NormColA{1}{r}\NormColA{2}{r}\ldots\NormColA{N}{r}$
		\label{line:Weight}
		\EndFor
		\For{$ \ell = 1,\ldots,s$}
		\State Sample $r$ with probability $w_r/\norm{\V{W}}{1}$
		\label{line:CorePartition}
		\For {$n = 1,...,N$}
		\label{line:ItemPartitionFor}
		\State Sample $i_n$ with probability $|\anr{n}{r}|/\norm{\ColVecA{n}}{1}$
		\EndFor
		\label{line:ItemPartitionEnd}
		\State
		\label{line:Scoring}
		$\score{\ell} \leftarrow sgn(\anr{1}{r}\cdots\anr{N}{r})$
		\If {$\V{i}=\coord$ has not been sampled}
		\State  Create $\predx \leftarrow \score{\ell} $
		\Else
		\State $\predx \leftarrow \predx + \score{\ell}$
		\EndIf
		\EndFor
		\label{line:ScoringEnd}
	\end{algorithmic}
\end{algorithm}

\subsubsection{Filtering}

Let $\Psi_p = \{\V{i}_p|p = 1,2,\ldots,P\}$ denote the set of unique index tuples that have been sampled. $P\leq s$ since some $\V{i}$ may be picked more than once. There are two strategies for identifying the $t$ largest entries from the samples. 

In the first one, we directly compute the exact entry value for each sample in $\Psi_{p}$ using \Eqn{CPValue} and then find the $t$ largest ones. Inspired by previous works\cite{BaPiKoSe15,Cohen97}, in the second strategy, we utilize the scores $\predx$ obtained during sampling. Since computing the exact entry value for all sampled indices would be time consuming when $R$ and $N$ are large, while the load for computing $\predx$ is much lighter. Moreover, as we show through theoretical analysis later, the expectation of $\predx$ is proportional to the exact value $x_{\V{i}}$. Therefore, we can use these estimated scores to filter out the relatively small ones and only compute the exact values for a subset of the sampled indices. Specifically, we denote the subset by $\Psi_{t'}$ that contains $t'$ index tuples corresponding to the top-$t'$ largest $\predx$, i.e. $\Psi_{t'} = \{ \V{i} | \predx \geq \hat{x}_{\V{i'}},\forall i'\in \Psi_{p} \backslash \Psi_{t'}\}$. Then the exact values are computed only for $\V{i}$ in $\Psi_{t'}$. And the $t$ index tuples with the largest $x_{\V{i}}$  in $\Psi_{t'}$ are extracted as the output of our algorithm.

The choice of $t'$ is a trade-off between computation and accuracy. And the recall, percentage of top-$t$ entries identified, of the first direct computing strategy is determined by the probability of largest entries(its index tuples) are sampled during each iteration. The higher the probabilities for the top entries, the more likely they will be sampled and thus the higher the recall. The second strategy will save computation. However, since the scores are only estimations of the true entry values, some orders between the entries may not be kept so that some top entries may be filtered out, which lead to a slightly lower recall than the first strategy.


\subsection{Theoretical Analysis}
In each iteration, we sample one node, indexed by $r$, from the core partition and $N$ nodes, indexed by $\boldsymbol{i}$, from $N$ peripheral partitions. Let $\epsilon_{\boldsymbol{i},r}$ denotes the event of choosing $r$ and $\boldsymbol{i}$, while $\epsilon_{\boldsymbol{i}}$ denotes that of choosing $\boldsymbol{i}$. We first analyze the probability $p(\epsilon_{\V{i}})$ that an index tuple to be sampled during each iteration and the expectation of estimation $\predx$ with an error bound on it.

\begin{lemma}\label{lem:CoreBasicProbability}
Suppose that all elements in the factor matrices $\M{A}^{(n)}, n\in[N]$ are nonnegative. Then we have	$p(\epsilon_{\V{i}})=x_{\V{i}}/\norm{\V{w}}{1}$.
\end{lemma}

\begin{IEEEproof}
The probability $p(\epsilon_{\V{i}})$ is simply the marginal distribution of $p(\epsilon_{\V{i},r})$, so we have:
	\begin{align*}
	p(\epsilon_{\V{i}})
	& = \sum_{r} p(\epsilon_{\V{i},r}) \\
	& = \sum_{r} p(r)\cdot p(i_1|r)\cdots p(i_N|r) \\
	& = \sum_{r} \frac{w_{r}}{\norm{\V{W}}{1}}
	\frac{|\anr{1}{r}|}{\NormColA{1}{r}} \ldots \frac{|\anr{N}{r}|}{\NormColA{N}{r}}\\
	& = \sum_{r} \frac{\anr{1}{r}\cdots\anr{N}{r}}{\norm{\V{W}}{1}}
	= \frac{x_{\V{i}}}{\norm{\V{w}}{1}}
	\end{align*}
\end{IEEEproof}

Let $c_{\V{i},\ell}$ be a random variable that $c_{\V{i},\ell}=\score{\ell}$ if $\V{i}$ is picked in the $\ell$-th iteration. Otherwise $c_{\V{i},\ell}=0$. Thus final score $\predx$ can be written as:
\begin{equation}\label{eq:CoreBasicScore}
\predx = \sum_{\ell=1}^{s} c_{\V{i},\ell}
\end{equation}

\begin{lemma}\label{lem:CoreBasicExpectation}
	The expectation of $\predx$ equals to $sx_{\V{i}}/\norm{\V{w}}{1}$.
\end{lemma}

\begin{IEEEproof}
	$c_{\V{i},\ell}$ are i.i.d for fixed $\V{i}$ and varing $\ell$.
	Thus the expectation of $\predx$ is:
	\begin{align*}
	\mathbb{E}[\predx]
	& = \sum_{\ell=1}^{s}\mathbb{E}[c_{\V{i},\ell}] = \sum_{\ell=1}^{s}\sum_{r} p(\epsilon_{\V{i},r})\score{\ell} \\
	& = s\sum_{r} \frac{|\anr{1}{r}\cdots\anr{N}{r}|}{\norm{\V{W}}{1}}
	sgn(\anr{1}{r}\cdots\anr{N}{r})\\
	& = s\sum_{r} \frac{\anr{1}{r}\cdots\anr{N}{r}}{\norm{\V{W}}{1}}
	= \frac{sx_{\V{i}}}{\norm{\V{w}}{1}}
	\end{align*}
\end{IEEEproof}

\subsubsection{Error Bounds}
The~\Lem{CoreBasicExpectation} shows that $\predx\norm{\V{w}}{1}/s$ is an estimation for $x_{\V{i}}$. Further, we derive the error bound on this estimation.
\begin{theorem}\label{theo:CoreBasicBound}
	Fix $\delta > 0$ and error probability $\sigma\in(0,1)$.
	Assuming all entries in the factor matrices are nonnegative.
	If the number of samples
	\[
	s \geq 3\norm{\V{w}}{1}\log{(2/\sigma)}/(\delta^2 x_{\V{i}})
	\]
	then
	\[
	Pr(|\predx\norm{\V{W}}{1}/s - x_{\V{i}}| > \delta x_{\V{i}}) \leq \sigma
	\]
\end{theorem}

\begin{IEEEproof}
	We follow the proof of Lemma 3 in~\cite{BaPiKoSe15}.
	Since  $c_{\V{i},1},\cdots,c_{\V{i},s}$
	are independent random variables taking values in $\{0,1\}$.
	So $\predx$ is the sum of independent Poisson trials(while Bernoulli trials are the special case).
	The Chernoff bounds on the sum of Poisson trials shows, for any $0 <\delta <1 $:
	\[
	Pr(|\predx - \mu|\geq\delta\mu) \leq 2\exp{(-\mu\delta^2/3)}
	\]
	where $\mu=\mathbb{E}[\predx]=sx_{\V{i}}/\norm{\V{W}}{1}$.
	And by the choice of $s$, we have
	$\mu\leq 3\log{(2/\sigma)/\delta^2}$.
	Then
	\[
	Pr(|\predx-sx_{\V{i}}/\norm{\V{W}}{1}|\geq\delta sx_{\V{i}}/\norm{\V{W}}{1}) \leq \sigma
	\]
	multiplying $\norm{\V{W}}{1}/s$ inside ${\rm Pr}[\cdot]$ gives
	\[
	Pr(|\predx\norm{\V{W}}{1}/s - x_{\V{i}}| > \delta x_{\V{i}}) \leq \sigma
	\]
\end{IEEEproof}

The~\Lem{CoreBasicProbability} suggests that the sampling process is a binomial distribution${\sim}B(s,p(\epsilon_{\V{i}}))$ with relatively small $p(\epsilon_{\V{i}})$ and large $s$. So the Poisson distribution with $\lambda = sp(\epsilon_{\V{i}})$ can be used to approximate the probability $Pr(x\geq 1)$, which is event for picking the index tuple $\V{i}$ at least once after all iterations. The approximation is $Pr(x\geq1) = 1-Pr(x=0)\approx 1-e^{-sp(\epsilon_{\V{i}})}$. If we fix some error probability with $\sigma \in (0,1)$ and takes the equality, it gives $1-e^{-sp(\epsilon_{\V{i}})} \geq 1-\sigma$. This suggests an approximate sample number with $s \geq \ln(1/\sigma)/p(\epsilon_{\V{i}})$ of finding the index tuple $\V{i}$ with error probability $\sigma$. And~\Theo{CoreBasicBound} also suggests that to get a reasonable estimation for $x_{\V{i}}$ (say $\sigma = 0.05,\delta=0.1$), we need the sample numbers to be around the level of $1000\times\norm{\V{w}}{1}/x_{\V{i}}$.

\section{Extension of the Basic Sampling Method}
%TODO
The recall we can obtain heavily depends on the probability distribution for sampling the entries ie. $p(\epsilon_{\V{i}})$. To achieve better performance, we focus on the improvement of this distribution by amplifying the odds for the top entries. Instead of sampling one node $r$ from the core partition, we pick one but compound node $(r,r')$ from some extended core partition, in which the number of nodes is expanded to $R^2$. Then conditioned on $(r,r')$, we sample $N$ nodes indexed by $(i_1,\ldots,i_N)$ from the peripheral partitions as depicted in ~\Fig{ExtensionOfCorePartition}. As a matter of course, the weight of edge $((r,r'),i_n)$, connecting the node $(r,r')$ and $i_n$, as well the weight of nodes in core partition, need to be redesigned so that the expectation of estimations will still meet the actual value.

\begin{figure}[!ht]
	\centering
	\includegraphics[]{ExtensionOfCorePartition}
	\caption{Extension of core partition. This is an extension with order $k=2$. And these compound nodes constitute a new core partition.}
	\label{fig:ExtensionOfCorePartition}
\end{figure}

More generally, we virtually create $R^k$ $k$-compound nodes,
indexed by $\V{r}=(r_1,\ldots,r_k)$.
These $k$-compound nodes constitute a new partition called Core$^k$ partition. And the algorithm starts by sampling nodes from the Core$^k$ partition. We name this strategy Core$^k$ sampling. The method discussed in previous section is a special case with $k=1$. 

To assign weight for $k$-compound nodes $\V{r}$ and probabilities of sampling peripheral nodes given $\V{r}$, we firstly consider the  weight of edge $(\V{r},i_n)$. The proposed formula for computing the edge weight is defined as followed:
\begin{equation}
\enr{n}{r} = \anr{n}{r_1}\cdots\anr{n}{r_k}
\end{equation}
and weights of $k$-compound nodes $\V{r}$ are analogous to those in basic sampling:
\begin{equation}
w_{\V{r}} = \NormColE{1}{r}\cdots\NormColE{N}{r}
\end{equation}
where $\V{e}_{*\V{r}}^{(n)}$ is the weight vector drawn from the $n$-th peripheral partition and its 1-norm is:
\begin{equation}
\NormColE{n}{r} = \sum_{i_n}\enr{n}{r}=\sum_{i_n}|\anr{n}{r_1}\cdots\anr{n}{r_k}|
\end{equation}

After an index tuple $\V{i}$ is sampled, a score is also computed for container $\predx$:
\begin{equation}
\score{\ell} \leftarrow sgn(\enr{1}{r}\cdots\enr{N}{r})
\end{equation}
The sampling procedure is similar with \Alg{CoreSampling} and we summarize it in \Alg{CorekSampling}. If we straighten the nodes in Core$^k$ partition to one dimension with $b(\V{r})$ indicating the positions, then Core$^k$ sampling strategy will be the same as basic algorithm acting on a set of new matrices, presumably, called $\M{E}^{(n)}$, $n\in[N]$. Then the $b(\V{r})$-th column of $\M{E}^{(n)}$ is $[e_{1,\V{r}}^{(n)},\cdots,e_{L_n,\V{r}}^{(n)}]^T$. This is an alternative explanation of the Core$^k$ sampling.

 In the following part, we prove that in Core$^k$ sampling, the probability an index tuple $\V{i}$ is sampled in on iteration is proportional to $x_{\V{i}}^k$ and the expectation of $\predx$ is also proportional to the $k$-th power of $x_{\V{i}}$. Therefore, in Core$^k$ extension, more focus will be put on the top entries.
\begin{algorithm}
	\caption{Core$^k$ sampling}
	\label{alg:CorekSampling}
	Given factor matrix $\FacMat{A}{n}\in \mathbb{R}^{L_n\times R}, n = 1,2,\ldots,N$.\\
	Let $s$ be the number of samples, $k$ for the extension order.$[R]^k$ denotes all the
	\begin{algorithmic}[1]
		\For{$\V{r}\in{\underbrace{[R]\times \cdots \times [R]}_{k}}$}
		\For{$n = 1,...,N$}
		\State $\NormColE{n}{r} \leftarrow \sum_{i_n}|\anr{n}{r_1}\cdots\anr{n}{r_k}|$
		\EndFor
		\State $w_{\V{r}} \leftarrow \NormColE{1}{r} \cdots \NormColE{N}{r} $
		\EndFor
		\For{$ \ell = 1,\ldots,s$}
		\State Sample $\V{r}$ with probability $w_\V{r}/\norm{\V{W}}{1}$
		\label{line:nodes}
		\For {$n = 1,...,N$}
		\State Sample $i_n$ with probability $|\enr{n}{r}|/\NormColE{n}{r}$
		\EndFor
		\State
		$\score{\ell} \leftarrow sgn(\enr{1}{r}\cdots\enr{N}{r})$
		\If {$\V{i}=\coord$ has not been sampled}
		\State  Create $\predx \leftarrow \score{\ell} $
		\Else
		\State $\predx \leftarrow \predx + \score{\ell}$
		\EndIf
		\EndFor
	\end{algorithmic}
\end{algorithm}

\subsection{Theoretical Analysis}

For Core$^k$ sampling, in each iteration we sample one compound node, indexed by $\boldsymbol{r}$, from the Core$^k$ partition and $N$ nodes, indexed by $\boldsymbol{i}$, from $N$ peripheral partitions. Let $\epsilon_{\boldsymbol{i},\boldsymbol{r}}$ denotes the event of choosing $\boldsymbol{r}$ and $\boldsymbol{i}$, $\epsilon_{\boldsymbol{i}}$ denote that of choosing $\boldsymbol{i}$. Similarly, we analyze the probability $p(\epsilon_{\V{i}})$ and the expectation of $\predx$.

\subsubsection{Probability of Coordinates}

\begin{lemma}\label{lem:Probability}
	Suppose that all elements of the factor matrices $\M{A}^{(n)}, n\in[N]$ are nonnegative. 
	In Core$^k$ sampling, we have $p(\epsilon_{\V{i}})$ equals to $x^k_{\V{i}}/\norm{\V{w}}{1}$.
\end{lemma}

\begin{IEEEproof}
	The probability $p(\epsilon_{\V{i}})$ is marginal distribution of $p(\epsilon_{\V{i},\V{r}})$,
	so we have:
	\begin{align*}
	p(\epsilon_{\V{i}})
	& = \sum_{\V{r}} p(\epsilon_{\V{i},\V{r}}) \\
	%& = \sum_{\V{r}} p({\rm pick\ }\V{r})p({\rm pick\ }i_1|\V{r})\cdots p({\rm pick\ }i_N|\V{r})\\
	& = \sum_{\V{r}} \frac{w_{\V{r}}}{\norm{\V{W}}{1}}
	\frac{|\Sca{e}{1}{\V{r}}|}{\norm{\ColVec{e}{n}{*\V{r}}}{1}}\ldots\frac{|\Sca{e}{N}{\V{r}}|}{\norm{\ColVec{e}{N}{*\V{r}}}{1}}\\
	& = \sum_{\V{r}} \frac{\Sca{e}{1}{\V{r}}\cdots\Sca{e}{N}{\V{r}}}{\norm{\V{W}}{1}}\\
	& = \frac{(\sum_{r}\anr{1}{r}\cdots\anr{N}{r})^k}{\norm{\V{W}}{1}}
	= \frac{x^k_{\V{i}}}{\norm{\V{w}}{1}}
	\end{align*}
\end{IEEEproof}

Let $c_{\V{i},\ell}$ be a random variable that if $\V{i}$ has been picked in the $\ell$-th iteration, $c_{\V{i},\ell}=\score{\ell}$. Otherwise $c_{\V{i},\ell}=0$. The final score $\predx$ can be written as
\begin{equation}\label{eq:CorekScore}
\predx = \sum_{\ell=1}^{s} c_{\V{i},\ell}
\end{equation}

\begin{lemma}\label{lem:Expectation}
	The expectation of approximation $\hat{x}_{\V{i}}$ in Core$^k$ sampling equals to $sx^k_{\V{i}}/\norm{\V{w}}{1}$.
\end{lemma}
\begin{IEEEproof}
	$c_{\V{i},\ell}$ are i.i.d for fixed $\V{i}$ and varing $\ell$. 
	The expectation of $\predx$ is:
	\begin{align*}
	\mathbb{E}[\predx]
	& = \sum_{\ell=1}^{s}\mathbb{E}[c_{\V{i},\ell}] = \sum_{\ell=1}^{s}\sum_{\V{r}} p(\epsilon_{\V{i},\V{r}})\score{\ell} \\
	& = s\sum_{\V{r}} \frac{|\enr{1}{r}\cdots\enr{N}{r}|}{\norm{\V{W}}{1}}
	sgn(\enr{1}{r}\cdots\enr{N}{r})\\
	& = s\sum_{\V{r}} \frac{\enr{1}{r}\cdots\enr{N}{r}}{\norm{\V{W}}{1}}\\
	& = s\frac{(\sum_{r}\anr{1}{r}\cdots\anr{N}{r})^k}{\norm{\V{W}}{1}}
	= \frac{sx^k_{\V{i}}}{\norm{\V{w}}{1}}
	\end{align*}
\end{IEEEproof}

\subsubsection{Error Bounds}

\begin{theorem}\label{theo:Bound}
	Fix $\delta > 0$ and error probability $\sigma \in (0,1)$.
	Assuming all entries in factor matrices are nonnegative.
	In Core$^k$ sampling, if the number of samples
	\[
	s \geq 3\norm{\V{w}}{1}\log{(2/\sigma)}/(\delta^2 x^k_{\V{i}})
	\]
	then
	\[
	Pr(|\predx\norm{\V{W}}{1}/s - x^k_{\V{i}}| > \delta x^k_{\V{i}}) \leq \sigma
	\]
\end{theorem}

\begin{IEEEproof}
	Since  $c_{\V{i},1},\cdots,c_{\V{i},s}$
	are independent random variables taking values in $\{0,1\}$.
	So $\predx$ is the sum of independent Poisson trials(Bernoulli trials are the special case).
	The Chernoff bounds on the sum of Poisson trials shows, for any $0 <\delta <1 $:
	\[
	Pr(|\predx - \mu|\geq\delta\mu) \leq 2\exp{(-\mu\delta^2/3)}
	\]
	where $\mu=\mathbb{E}[\predx]=sx^k_{\V{i}}/\norm{\V{W}}{1}$ in Core$^k$ sampling.
	And by the choice of $s$, we have
	$\mu\leq 3\log{(2/\sigma)/\delta^2}$.
	Then
	\[
	Pr(|\predx-sx^k_{\V{i}}/\norm{\V{W}}{1}|\geq\delta sx^k_{\V{i}}/\norm{\V{W}}{1}) \leq \sigma
	\]
	multiplying $\norm{\V{W}}{1}/s$ inside ${\rm Pr}[\cdot]$ gives
	\[
	Pr(|\predx\norm{\V{W}}{1}/s - x^k_{\V{i}}| > \delta x^k_{\V{i}}) \leq \sigma
	\]
\end{IEEEproof}

%Note that in the diamond sampling method for matrix~\cite{BaPiKoSe15},
%they also sampled two nodes from the core partition.
%However, the second node $r'$ was sampled after the indices $i_1$ and $i_2$ were picked
%and none extension of the factor matrices were introduced.
%As a result, only the estimation $\predx$ was improved
%but not the probability of picking the top entries.

\subsection{Sampling for Multiple Users}

In this subjection, we present an algorithm for efficiently identifying top entries in sub-arrays of a tensor, which is of particular interest for recommender systems. Without loss of generality, we assume that the first factor matrix $\FacMat{A}{1}$ corresponds to all existing users with each row characterizing a single user. For one user $\V{u} = \RowVecA{1}$ we are interested in top entries in the (N-1)-order tensor $\T{X}_\V{u}$, whose CP decomposition is
\begin{align}
\label{eq:UserCP}
\T{X}_{\V{u}} &\approx \KT{\V{u},\FacMat{A}{2},\cdots,\FacMat{A}{N}} \\ \notag
&=\sum_{r=1}^Ru_r\cdot\ColVecA{2}\circ\cdots\circ\ColVecA{N}
\end{align}

To generate recommendations for multiple users, an naive approach is to run the above sampling algorithm independently for different users. One can blend $\V{u}$ into other factor matrices so that it becomes a general form or takes $\V{u}$ as a special factor matrix. However, this is totally unnecessary. We find that if we take the $\V{u}$ as a special factor matrix, for different users, only the probabilities for sampling nodes from the core partition are different. Since the peripheral partition of $\V{u}$ is a special case with one node leaking the influence only to core partition. In the following step, different users share the same distribution for sampling the other peripheral nodes. In expectation, the difference between users is how many index tuples need to sample conditioned on each compound nodes. 

Based on this observation, for each node in the core partition, we build $R^k$ pool each containing indices $\V{i}=(i_2,\ldots,i_N)$ that have been picked for a given $\V{r}$. For a new user, when a core node $\V{r}$ is chosen, we can directly use the indexes tuples kept in its pool and only sample new $\V{i}$ when necessary. By sharing among users, a huge number of samples can be saved. And the procedure is illustrated in~\Alg{MultiUsersSampling}.

\begin{algorithm}
	\caption{Finding top-$t$ entries for multiple users}
	\label{alg:MultiUsersSampling}
	Given factor matrix $\FacMat{A}{n}\in \mathbb{R}^{L_n\times R}, n = 1,2,\ldots,N$.
	$\FacMat{A}{1}$ is the set of querying users.
	Let $s$ be the number of samples, and $m=R^k$.
	\begin{algorithmic}[1]
		\State Initialize $m$ empty pools $\V{g}_1,\V{g}_2,\ldots,\V{g}_m$
		\State Initialize $f_{\V{r}} = 0$ for all compound nodes $\V{r}$.
		\For {$i_1 =1,2,\ldots,L_1$}
		\State Let the current user be $\V{u}=\RowVecA{1}$
		\ForAll{$\V{r}=(r_1,\ldots,r_k)$}
		\State $w_\V{r} \leftarrow |u_{r_1}\cdots u_{r_k}|\NormColE{2}{r} \cdots \NormColE{N}{r}$
		\EndFor
		\For {$\ell = 1,\ldots,s$}
		\State Sample $\V{r}$ with probability $w_\V{r}/\norm{\V{W}}{1}$.
		\label{line:Indexes}
		\State  Increment $f_{\V{r}}$.
		\EndFor
		\ForAll {$\V{r}$}
		\If {$f_\V{r} > |\V{g}_\V{r}|$ }
		\State Sample $f_{\V{r}} - |\V{g}_{\V{r}}|$ index tuples $\V{i}$ into $\V{g}_{\V{r}}$.
		\EndIf
		\State Use $f_{\V{r}}$ index tuples $\V{i}$ in $\V{g}_{\V{r}}$ and compute scores.
		\EndFor
		\State Post-processing for finding top-$t$ entries of $\V{u}$.
		\EndFor
	\end{algorithmic}
\end{algorithm}

\section{Implementation Details}

In this section, we discuss some implementation details that could improve the performance.
Follow the previous work\cite{Cohen97}, instead of picking a (compound) node from the Core(Core$^k$) partition, we directly get $s$ core nodes in one loop. We compute the expected number $\mu_{\V{r}}=sw_\V{r}/\norm{\V{w}}{1}$ for each (compound) nodes. And sample the count $f_\V{r}$ :
\begin{equation}f_\V{r}=
\left\{
\begin{array}{ll}
	\lfloor \mu_{\V{r}} \rfloor, & \hbox {with probability $p=\lceil \mu_{\V{r}} \rceil - \mu_{\V{r}}$}  \\
	\lceil \mu_{\V{r}} \rceil,   & \hbox{with probability $p=\lfloor \mu_{\V{r}} \rfloor - \mu_{\V{r}}$}
\end{array}
\right.
\end{equation}
so that it will keep the expected value of $\mu_{\V{r}}$. This requires $O(R^k)$ work while sampling $\V{r}$ $s$ times requires $O(sk\log R)$ work.

To conduct Core$^k$ sampling, utilizing the extension matrices explanation, we extend the factor matrices to $\FacMat{E}{n}$, which requires extra $O(L_nR^k)$ in space and $O(R^k)$ in computation for each $n$ in $[N]$. This will be costly in storage when $k$ is large. Using the above strategy, we have obtained the number of times $f_\V{r}$ for each core node appears. Given a core node, the sampling of the peripheral nodes will only depend on one column in $\FacMat{E}{n}$ that is indexed by $b(\V{r})$. Therefore, instead of storing the whole $\FacMat{E}{n}$, we only keep one column of it at a time. This requires extra $O(L_n)$ space and $O(2R^k)$ computation for each $n$ in $[N]$, which makes the high order possible. When k is small, we can directly store $\FacMat{E}{n}$ in exchange for speed.

\section{Experiments}

We conducted several experiments to test the effectiveness of our sampling algorithms on several real world data sets. Four for tag recommendation application: Delicious Bookmarks\footnote{http://www.delicious.com}(Delicious), Last.FM\footnote{http://www.lastfm.com}(LastFM), MovieLens\footnote{http://www.grouplens.org}+IMDb\footnote{http://www.imdb.com}/Rotten Tomatoes\footnote{http://www.rottentomatoes.com}(ML-S), and a larger MovieLens data(ML-L). The first three are from~\cite{Cantador:RecSys2011} and the forth one is froms~\cite{Harper2015}. Beside, we have one for link prediction: dblp\footnote{http://dblp.uni-trier.de/db/}. More details about those data sets will be introduced in the subsequent subsections.

\subsection{Data Preprocessing}

% TODO : check Schke2008Tag for core-based approach
These data sets for tag recommendation are some tuples of $(u,i,t)$, which is a coordinate in tensor with each one means the $u$-th user has tagged the $i$-th item with $t$-th tag. Following previous practice\cite{Schke2008Tag}, a $p$-core processing with $p=5$ is used so that each user, item and tag occurs at least 5 tuples. And for link prediction, the actual tuple used is $(u,i,r^t_{ui})$, which means the total number of papers $r^t_{ui}$ by author $u$ at conference $i$ in year $t$.

% TODO more citetions for BPTF
We use the Bayesian Probabilistic Tensor Factorization (BPTF) approach to do collaborative filtering and optimize the pair-wise ranking function of Bayesian Personalized Ranking (BPR)~\cite{Rendle_BPR,Rendle_RTF} for tag recommendation data sets. The factor matrices are learned so that the observed entries have ranking scores that are higher than the unobserved ones. $R$ is set to 64 for tag recommendation. For dblp data set we follow \cite{AcarDunlavy09} to extract only the inproceedings from 1991 to 2007 and the same Alternating Least
Squares (ALS) approach using the Tensor Toolbox\cite{TTB_Software,TTB_Sparse} for CP model is used with $R=50$.

After getting the factor matrices, we run different algorithms to find the top entries. Some statistics of the data sets after preprocessing are shown in \Table{Data}.

%TODO rewrite the table
\begin{table*}[!t]
	% increase table row spacing, adjust to taste
	\renewcommand{\arraystretch}{1.3}
	% if using array.sty, it might be a good idea to tweak the value of
	% \extrL_aheight as needed to properly center the text within the cells
	\caption{Statistics for each data set}
	\label{table:Data}
	\centering
	% Some packages, such as MDW tools, offer better commands for making tables
	% than the plain LaTeX2e tabular which is used here.
	\begin{tabular}{c|c|c|c|c|c}
	\hline
		 DataSet  &    $\|\V{w}^1\|_1$     &    $\|\V{w}^2\|_1$     &    $\|\V{w}^3\|_1$     & Top-$1$  &  Top-1k  \\
	\hline
		  ML-S    & $1.3509\times 10^{9}$  & $7.3067\times 10^{9}$  & $6.4865\times 10^{10}$ & 81.5643  & 43.3850  \\
		  ML-L    & $9.8383\times 10^{9}$  & $4.8203\times 10^{10}$ & $3.6893\times 10^{11}$ & 88.7368  & 50.5948  \\
		 LastFM   & $6.8660\times 10^{10}$ & $6.4091\times 10^{10}$ & $9.4795\times 10^{12}$ & 234.7302 & 101.7950 \\
		Delicious & $2.5191\times 10^{11}$ & $1.2185\times 10^{12}$ & $8.6824\times 10^{12}$ & 95.4723  & 40.8289  \\
		  dblp    &  $1.1183\times10^{7}$  &  $3.8655\times10^{6}$  & $7.9704\times 10^{6}$  &  4.1895  &  2.4516  \\
	\hline
	\end{tabular}
\end{table*}

\subsection{Accuracy and Time}

We present time and accuracy results for the each data set. To evaluate accuracy for finding the top-$t$ largest elements in prediction tensor, we plot recall, i.e. the percentage of top-$t$ entries identified, versus the number of samples. Our algorithm focus on the effectiveness of finding those values. The recall for $t\in\{1,10,100,1000\}$ as well as the running time are shown for each data sets . The time for exhaustive computing is also drawn as the baseline. Here we set $t'=s$ and focus on the performance of the sampling stage. For each $s$, we run the sampling algorithms 10 times and the average is reported.

\begin{figure}[!tbh]
	\centering
	\subfloat[]{
		\includegraphics[width=3.3in]{img/ML_S_BasicRecall}
		\label{fig:BasicRecall:ML-S}
	}\\
	\subfloat[]{
		\includegraphics[width=3.3in]{img/ML_L_BasicRecall}
		\label{fig:BasicRecall:ML-L}
	}\\
	\subfloat[]{
		\includegraphics[width=3.3in]{img/LastFM_BasicRecall}
		\label{fig:BasicRecall:LastFM}
	}\\
	\subfloat[]{
		\includegraphics[width=3.3in]{img/Delicious_BasicRecall}
		\label{fig:BasicRecall:Delicious}
	}\\
	\subfloat[]{
	\includegraphics[width=3.3in]{img/dblp_BasicRecall}
	\label{fig:BasicRecall:dblp}
	}
	\caption{Recall of Core$^k$ sampling for $k\in\{1,2,3\}$.}
	\label{fig:BasicRecall}
\end{figure}

\begin{figure}[!tbh]
	\centering
	\subfloat[]{
		\includegraphics[width=1.7in]{img/ML_S_BasicTime}
		\label{fig:BasicTime:ML-S}
	}
	\subfloat[]{
		\includegraphics[width=1.7in]{img/ML_L_BasicTime}
		\label{fig:BasicTime:ML-L}
	}\\
	\subfloat[]{
		\includegraphics[width=1.7in]{img/LastFM_BasicTime}
		\label{fig:BasicTime:LastFM}
	}
	\subfloat[]{
		\includegraphics[width=1.7in]{img/Delicious_BasicTime}
		\label{fig:BasicTime:Delicious}
	}\\
	\subfloat[]{
	\includegraphics[width=1.7in]{img/dblp_BasicTime}
	\label{fig:BasicTime:dblp}
	}
	\caption{Running time for the experiments in~\Fig{BasicRecall}.}
	\label{fig:BasicTime}
\end{figure}

\subsubsection{MovieLens}
Here we have two data sets of MovieLens, ML-S and a larger one ML-L. ML-S contains $L_1 = 456$ users, $L_2 = 1,973$ items and $L_3 = 1,222$ tags and ML-L consists of $L_1 = 993$ users, $L_2 = 3,298$ items and $L_3 = 2,555$ tags. The most used latent dimension in practice is $R=\{16,32,64\}$ since the complexity of model with large $R$ may over-fits the training set and takes more time to train. So we use $R=64$ for the all tag recommendation data sets.

The performances of recall on ML-S and ML-L are shown in~\Fig{BasicRecall:ML-S} and~\Fig{BasicRecall:ML-L}. And~\Fig{BasicTime:ML-S} and \Fig{BasicTime:ML-L} show the corresponding running time. The results show that by extending the core partition to Core$^2$ and Core$^3$, we can get the same level of recalls with much fewer samples. In Core$^2$ we store the extension matrices for speeding up.

As show in our theoretical analysis, approximate sample number $s\geq\ln(1/\sigma)/p(\epsilon_{\V{i}})$ is demand for finding $x_\V{i}^*$, where $x_\V{i}^*$ is the reconstructed value at $\V{i}$ with absolute of original factor matrices. Since $x_\V{i}^*\geq x_\V{i}$ and $p(\epsilon_{\V{i}})=\norm{\V{w}}{1}/x_\V{i}^*$, we use $x_\V{i}$ to do evaluation on all data set. If we set $\sigma=0.1$, the least sample number for \{Core$^1$,Core$^2$,Core$^3$\} on finding the top-$1000$ are \{$7.1697\times10^7,8.9384\times10^6,1.8290\times10^6$\}for ML-S and $\{4.4774\times10^8,4.3359\times10^7,6.5591\times10^6\}$ for ML-L. This is consistent with our theoretical analysis that in Core$^k$ sampling, the focus on top entries is amplified, making them more probable to be sampled. All extensions are orders of magnitude faster than exhaustive computing.

The running time costs more for Core$^3$ than Core$^2$ and Core$^1$. And with increasing of sample numbers high order extension may outperform in both recall and time-consuming. This is because although the algorithm with high order extension takes much more time in computing the extension matrices or probability vectors. With high order extension the probabilities for larger entities are emphasized, causing the smaller size of $\Psi_p$ which saving the running time in filtering phase.

\subsubsection{LastFM}

LastFM has $L_1 = 1,348$ users, $L_2 = 6,927$ items and $L_3 = 2,132$ tags. The performance on LastFM is shown in~\Fig{BasicRecall:LastFM} and~\Fig{BasicTime:LastFM}. This also shows the consistent results on the effectiveness of our algorithms. Beside we use the LastFM data set to show more details in running time. For convenience, we separate its running time into four simple phases: initialization, sampling, scoring and filtering. Initialization stage consists the preprocessing for factor matrices and computing the weight of nodes in core partition. The sampling stage is for sampling indices which may including computing the probability distribution of the peripheral partition nodes when extension matrices is not stored previously. The scoring phase shows the time consumed for scoring each sampled index tuples and saving it into containers. The last processing phase is filtering the top-$t$ entities within all sampled index tuples. We plot the result in~\Fig{BasicTimeComp}.
\begin{figure}[!ht]
	\centering
	\subfloat[Sampling wiht $s=1e6$]{
		\includegraphics[width=1.7in]{LastFM_BasicTimeComp_S}
		\label{fig:BasicTimeComp:S}
	}
	\subfloat[Sampling wiht $s=1e8$]{
		\includegraphics[width=1.7in]{LastFM_BasicTimeComp_L}
		\label{fig:BasicTimeComp:L}
	}
	\caption{Running time for in each phases on LastFM data set.}
	\label{fig:BasicTimeComp}
\end{figure}

The running time for initialization is independent of sample numbers. As we can see in~\Fig{BasicTimeComp:S}, when $s$ is small Core$^3$ takes much more time in initialization and sampling stage and the extra time costing in sampling stage is due to the computation for probability vectors which takes extra $O(R^3)$ than using extension matrices for saving storage. With the growth of sample numbers, the initialization costs is much feeble and Core$^3$ saves time in scoring and filtering stages causing in the smaller set of $\Psi_p$.

\subsubsection{Delicious}
Delicious has $L_1 = 1,681$ users, $L_2 = 29,540$ items and $L_3 = 7,251$ tags.The performance on Delicious is shown in~\Fig{BasicRecall:Delicious} and~\Fig{BasicTime:Delicious}. And we also set $\sigma=0.1$ to compute the demand sample number for each algorithm. It shows $\{1.4207\times10^{10},1.6831\times10^9,2.9373\times10^8\}$ is demanded for finding the top-$1000$ in algorithm \{Core$^1$,Core$^2$,Core$^3$\}. 

\subsubsection{dblp}

dblp has $L_1 = 43,928$ authors, $L_2 = 2,572$ conferences and $L_3 = 17$ year slices. The high order extension also shows its ability to enhance the results. And for small data set, a low order extension algorithm is enough to handle.

\subsection{Use Prefiltering}
Since the score is used to estimate $x_\V{i}$, $\predx$ can be used to save the running time on filtering phase. We then evaluate the performance of using $\predx$ for prefiltering with $t'=s/10$. We show results on the LastFM data set in~\Fig{PrefilterRecallLastFM} and~\Fig{PrefilterTimeLastFM}. We notice that prefiltering leads to a slightly lower recall but less computation. Prefiltering save a mount of time since the load for $\predx$ is much lighter than computing all $x_\V{i}$ in $\Psi_p$. A slightly more time costs in scoring phase of prefiltering strategy is result of skipping the computation of scores $\score{\ell}$ when direct computing is conducted.

\begin{figure}[!ht]
	\centering
	\includegraphics[width=3.3in]{./img/PrefilterRecallLastFM}
	\caption{Compare to the recall of using prefiltering.}
	\label{fig:PrefilterRecallLastFM}
\end{figure}
\begin{figure}[!ht]
	\centering
	\subfloat[a]{\includegraphics[width=3in]{./PrefilterTimeLastFM_a}}\qquad
	\subfloat[b]{\includegraphics[width=3in]{./PrefilterTimeLastFM_b}}\qquad
	\subfloat[a]{\includegraphics[width=3in]{./PrefilterTimeLastFM_c}}
	\caption{Recall and time of using prefiltering.
		(a) Running time at $s=1e6$. (b) Running time at $s=1e7$. (c) Running time at $s=1e8$.}
	\label{fig:PrefilterTimeLastFM}
\end{figure}

\subsection{Sampling for Collaborative Filtering}

In~\Fig{MutilUsers}, we show the performance of~\Alg{MultiUsersSampling} applied for collaborative filtering, where the samples are shared among different users. Here Core$^2$ sampling is used to find the top-100 largest entries for the LastFM data set and $s$ is set to $10^6$. We can see that only for the first few users we need some time to get the samples and build the sample pools. After that, the computation time for a new user is much reduced.
\begin{figure}[!ht]
	\centering
	\includegraphics[width=2.7in]{./LastFM_MultiUsers}\\
	\caption{Recall and time results of~\Alg{MultiUsersSampling} on the LastFM data set.}
	\label{fig:MutilUsers}
\end{figure}

\subsection{Comparison with Diamond Sampling}

We also compare our algorithms with diamond sampling~\cite{BaPiKoSe15} for finding the top entries in the product of two matrices. For each user, we multiply the vector into the item matrix to get a user-oriented item matrix. The problem is to identify the top entries in the product of the item matrix and the tag matrix. Note that the searchings for different users are conducted independently, i.e. we run~\Alg{CorekSampling} instead of~\Alg{MultiUsersSampling} for this experiment. 
\begin{figure}[!ht]
	\centering
	\includegraphics[width=3in]{./ML_L_CompDiamondRecall}\\
	\caption{Comparison with Diamond sampling for the ML-L data set.}
	\label{fig:Comparison_recall}
\end{figure}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=2.7in]{./ML_L_CompDiamondTime}\\
	\caption{Running time for Diamond, Core$^1$, Core$^2$ and Core$^3$.}
	\label{fig:Comparison_time}
\end{figure}
We show the recalls and the time consumed by the sampling algorithms for the ML-L data set in~\Fig{Comparison_recall} and~\Fig{Comparison_time}, in which $t'$ is set to $s/10$. The performance of diamond sampling is better than Core$^1$ sampling. This is reasonable since the score $\hat{x}_{\V{i}}$ that the diamond sampling computed is proportional to the square of the entry magnitude. However, Core$^2$ and Core$^3$ extensions achieve higher recalls than diamond sampling with the same number of samples. This illustrates the advantage of augmenting the sampling probability to higher power of the entry magnitude.

\subsection{Accuracy of the Scores}

Since the $\predx$ is an estimation of $sx^k_{\V{i}}/\norm{\V{w}}{1}$ in Core$^k$ extension. We experimentally validate this analysis. We test on the ML-S and LastFM data set with $k=3$ and $s=10^8$. And the pairs $(x^3_{\V{i}},\norm{\V{w}}{1}\hat{x}_{\V{i}}/s)$ is plotted in~\Fig{MLSEstimation} and ~\Fig{MLSEstimation}. In both only the $10^4$ pairs with largest $x^3_{\V{i}}$ in sampled index tuple set $\Psi_p$ are shown. 
\begin{figure}[!ht]
	\centering
	% Requires \usepackage{graphicx}
	\includegraphics[width=3in]{./ML_S_Estimation}\\
	\caption{Plot of pairs $(\norm{\V{w}}{1}\hat{x}_{\V{i}}/s,x^3_{\V{i}})$ on ML-S.
		The solid line is the reference for equality.}
	\label{fig:MLSEstimation}
\end{figure}
\begin{figure}[!ht]
	\centering
	% Requires \usepackage{graphicx}
	\includegraphics[width=3in]{./LastFM_Estimation}\\
	\caption{Plot of pairs $(\norm{\V{w}}{1}\hat{x}_{\V{i}}/s,x^3_{\V{i}})$ on LastFM.
		The solid line is the reference for equality.}
	\label{fig:LastFMEstimation}
\end{figure}

By~\Theo{Bound}, if sample number $s \geq 3\norm{\V{w}}{1}\log{(2/\sigma)}/(\delta^2 x^k_{\V{i}})$ we can get $Pr(|\predx\norm{\V{W}}{1}/s - x^k_{\V{i}}| > \delta x^k_{\V{i}}) \leq \sigma$. We fix $\sigma=0.1$, at the result of $s=1e8$, we compute each $\delta$ for $x^k_{\V{i}}$ use the equality. And two dashed lines for $x^k_{\V{i}}\pm\delta x^k_{\V{i}}$ is also plotted. As expected, the points concentrate around the diagonal and most pairs fall within the two dashed lines, which confirmed with the theoretical result.


% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%


%\appendices
%\section{Proof of the First Zonklar Equation}
%Appendix one text goes here.

% you can choose not to have a title for an appendix
% if you want by leaving the argument blank
%\section{}
%Appendix two text goes here.


% use section* for acknowledgment
\ifCLASSOPTIONcompsoc
  % The Computer Society usually uses the plural form
%  \section*{Acknowledgments}
\else
  % regular IEEE prefers the singular form
%  \section*{Acknowledgment}
\fi


%The authors would like to thank...


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
\bibliography{IIP}
\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
%\begin{thebibliography}{1}
%
%\bibitem{IEEEhowto:kopka}
%H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
%  0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.
%
%\end{thebibliography}

% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

%\begin{IEEEbiography}{Michael Shell}
%Biography text here.
%\end{IEEEbiography}
%
%% if you will not have a photo at all:
%\begin{IEEEbiographynophoto}{John Doe}
%Biography text here.
%\end{IEEEbiographynophoto}
%
%% insert where needed to balance the two columns on the last page with
%% biographies
%%\newpage
%
%\begin{IEEEbiographynophoto}{Jane Doe}
%Biography text here.
%\end{IEEEbiographynophoto}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks
\end{document}
